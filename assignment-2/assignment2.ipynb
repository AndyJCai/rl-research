{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/IPython/core/magics/pylab.py:159: UserWarning: pylab import has clobbered these variables: ['gamma']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  warn(\"pylab import has clobbered these variables: %s\"  % clobbered +\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windy grid\n",
    "rewardSize = -1\n",
    "gamma = 0.95 # discount rate\n",
    "\n",
    "alpha = 0.85 # learning rate\n",
    "epsilon = 0.9 # e-greedy \n",
    "episodeNum = 1000 # number of episodes \n",
    "maxSteps = 100\n",
    "\n",
    "#Initializing the Q-matrix\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_gridworlds\n",
    "from lib.envs.windy_gridworld import WindyGridworldEnv\n",
    "\n",
    "env = WindyGridworldEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to choose the next action \n",
    "def choose_action(state): \n",
    "    action=None\n",
    "    if np.random.uniform(0, 1) < epsilon: \n",
    "        action = env.action_space.sample()\n",
    "    else: \n",
    "        action = np.argmax(Q[state, :]) \n",
    "    return action "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/montecarlo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_update():\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "@deprecate\n",
    "def SARSA_argmax(Q_s: defaultdict):\n",
    "    max_Q = float('-inf')\n",
    "    arg_max = -1\n",
    "    for k, v in Q_s.items():\n",
    "        if (v > max_Q):\n",
    "            max_Q = v\n",
    "            arg_max = k\n",
    "            \n",
    "    return arg_max\n",
    "\n",
    "#Function to learn the Q-value \n",
    "def SARSA_update(state, state2, reward, action, action2): \n",
    "    predict = Q[state, action] \n",
    "    target = reward + gamma * Q[state2, action2] \n",
    "    Q[state, action] = Q[state, action] + alpha * (target - predict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-19.97685624, -19.9774206 , -19.9725986 , -19.97514328],\n",
       "       [-19.97893946, -19.97702643, -19.97878283, -19.97898801],\n",
       "       [-19.97368909, -19.97823996, -19.97577503, -19.97106866],\n",
       "       [-19.97832121, -19.5887956 , -19.97057134, -19.97301117],\n",
       "       [-19.87419082, -19.647426  , -19.88619092, -19.97679166],\n",
       "       [-19.96689821, -19.46717115, -19.97326736, -19.97305382],\n",
       "       [-18.62493244, -19.7662756 , -19.14451742, -19.94134039],\n",
       "       [-19.14380975, -19.26732169, -19.04444932, -19.80683389],\n",
       "       [-19.67755593, -19.1329128 , -19.42151696, -19.63123005],\n",
       "       [-19.32861394, -18.71881735, -18.53200132, -18.60167478],\n",
       "       [-19.97427766, -19.97731815, -19.97500772, -19.97080778],\n",
       "       [-19.97827489, -19.95059684, -19.97092278, -19.96468425],\n",
       "       [-19.96644464, -19.97079765, -19.97170383, -19.97659339],\n",
       "       [-19.96044331, -19.8384884 , -19.97660548, -19.96938234],\n",
       "       [-19.94898105, -19.95369289, -19.96332993, -19.97397535],\n",
       "       [-19.93441353, -19.86756336, -19.85670286, -19.97465031],\n",
       "       [-19.80587224, -19.82625893, -19.85344842, -19.84417043],\n",
       "       [-19.77953937, -19.84271474, -19.8018949 , -19.84147357],\n",
       "       [-19.28433927, -19.85346179, -19.75588042, -18.34550307],\n",
       "       [-19.35092777, -19.36179787, -19.28591968, -19.63958485],\n",
       "       [-19.97423718, -19.97358816, -19.97774426, -19.96564178],\n",
       "       [-19.97250839, -19.97736231, -19.97077134, -19.97081185],\n",
       "       [-19.96774435, -19.96197726, -19.96485615, -19.97792854],\n",
       "       [-19.9322189 , -19.94342791, -19.9659006 , -19.96400034],\n",
       "       [-19.96900877, -19.73244979, -19.93888328, -19.96642158],\n",
       "       [-19.75245686, -19.7297961 , -19.50036852, -19.92119471],\n",
       "       [-19.75531707, -19.55217278, -19.86452615, -19.85971117],\n",
       "       [-19.88142999, -19.85754626, -19.68922834, -19.88130399],\n",
       "       [-18.87118639, -19.7535511 , -19.68893454, -19.80949481],\n",
       "       [-19.53347753, -19.22597867, -17.95774845, -19.76163587],\n",
       "       [-19.9777039 , -19.95934554, -19.96455385, -19.97601539],\n",
       "       [-19.97680688, -19.92675469, -19.96470706, -19.97169641],\n",
       "       [-19.95886045, -19.96277472, -19.96289672, -19.97203431],\n",
       "       [-19.96119893, -19.9591345 , -19.96883728, -19.96223315],\n",
       "       [-19.96718346, -19.86587244, -19.91633058, -19.95471231],\n",
       "       [-19.81847253, -19.65612893, -19.42068626, -19.94528815],\n",
       "       [-19.5142031 , -18.16450493, -17.10441555, -19.68301224],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [-19.83195608, -19.66284961, -19.76147606, -19.63002927],\n",
       "       [-16.18697837, -19.49018742,  -9.1372983 , -19.56520613],\n",
       "       [-19.9688117 , -19.96408304, -19.96134922, -19.9682504 ],\n",
       "       [-19.97113797, -19.95958298, -19.93246421, -19.96426542],\n",
       "       [-19.96786895, -19.92960812, -19.87226878, -19.96434536],\n",
       "       [-19.93334148, -19.77062756, -19.93239062, -19.96482427],\n",
       "       [-19.9412529 , -18.6816605 , -19.92887422, -19.88622061],\n",
       "       [-19.88835762, -17.32388003,  -7.34710493, -19.88410529],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        , -19.38622748,  -0.9775    , -18.6679611 ],\n",
       "       [-19.39474502, -19.23478739, -19.39191237,  -1.        ],\n",
       "       [ -9.92096208, -12.07067404, -15.20777785, -17.18741127],\n",
       "       [-19.95595544, -19.91806999, -19.96435086, -19.96046366],\n",
       "       [-19.96352379, -19.92292652, -19.95781989, -19.96430593],\n",
       "       [-19.96221893, -19.93363826, -19.94378259, -19.96474849],\n",
       "       [-19.91718029, -19.88439284, -19.89229668, -19.96520024],\n",
       "       [-19.94400132, -19.17856605, -19.88441106, -19.87477969],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [-16.53155182,   0.        ,   0.        ,   0.        ],\n",
       "       [-19.41230908,  -5.6402337 , -16.92100885, -14.65148382],\n",
       "       [ -5.37218759, -16.80084362,  -6.16446308,  -4.41386961],\n",
       "       [-19.94949428, -19.93422909, -19.96752463, -19.96782546],\n",
       "       [-19.96008934, -19.9477163 , -19.95829111, -19.96359291],\n",
       "       [-19.86944035, -19.94340915, -19.9447889 , -19.92686713],\n",
       "       [-19.94945417, -19.87899201, -19.77591345, -19.86708043],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        , -10.85619156,  -0.85      ,  -0.85      ],\n",
       "       [-13.50194847,  -5.43038414,  -7.66452145,  -1.55287844]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def SARSA_0():\n",
    "    #Initializing the reward  \n",
    "    reward=0\n",
    "\n",
    "    # Starting the SARSA learning \n",
    "    for episode in range(episodeNum): \n",
    "        t = 0\n",
    "        state1 = env.reset()\n",
    "        action1 = choose_action(state1) \n",
    "\n",
    "\n",
    "        while t < maxSteps: \n",
    "            #Getting the next state \n",
    "            state2, reward, done, info = env.step(action1) \n",
    "\n",
    "            #Choosing the next action \n",
    "            action2 = choose_action(state2) \n",
    "\n",
    "            #Learning the Q-value \n",
    "            SARSA_update(state1, state2, reward, action1, action2) \n",
    "\n",
    "            state1 = state2 \n",
    "            action1 = action2 \n",
    "\n",
    "            #Updating the respective vaLues \n",
    "            t += 1\n",
    "            reward += rewardSize\n",
    "\n",
    "            #If at the end of learning process \n",
    "            if done: \n",
    "                break\n",
    "\n",
    "    return Q\n",
    "SARSA_0()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to learn the Q-value \n",
    "def SARSA_update(state, state_n, reward, action, action_n): \n",
    "    predict = Q[state, action] \n",
    "    target = reward + gamma * Q[state2, action2] \n",
    "    \n",
    "    Q[state, action] = Q[state, action] + alpha * (target - predict) \n",
    "    \n",
    "def SARSA_update(state1, state_n, n, action, action_n):\n",
    "    predict = Q[state1, action1]\n",
    "    target = gamma**n * Q[state_n, action_n]\n",
    "    n -= 1\n",
    "    for i in range(n):\n",
    "        state_new, reward, done, info = env.step(action1)\n",
    "        action_new = choose_action(state_new)\n",
    "        action1 = action_new\n",
    "        target += gamma**i * reward\n",
    "        \n",
    "    Q[state, action] = Q[state, action] + alpha * (target - predict) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://miro.medium.com/max/1400/1*-v5wbqLYCvzrOQE2Zmv05g.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SARSA_n_old(n):\n",
    "    #Initializing the reward  \n",
    "    reward=0\n",
    "\n",
    "    # Starting the SARSA learning \n",
    "    for episode in range(episodeNum): \n",
    "        t = 0\n",
    "        state1 = env.reset()\n",
    "        action1 = choose_action(state1) \n",
    "        \n",
    "        while t < maxSteps: \n",
    "            #Getting the next state \n",
    "            state2, reward, done, info = env.step(action1) \n",
    "\n",
    "            #Choosing the next action \n",
    "            action2 = choose_action(state2) \n",
    "\n",
    "            #Learning the Q-value \n",
    "            SARSA_update(state1, state2, n, action1, action2) \n",
    "\n",
    "            state1 = state2 \n",
    "            action1 = action2 \n",
    "\n",
    "            #Updating the respective vaLues \n",
    "            t += 1\n",
    "            reward += rewardSize\n",
    "\n",
    "            #If at the end of learning process \n",
    "            if done: \n",
    "                break\n",
    "\n",
    "    return Q "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SARSA_n(n):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "SARSA_n() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-bb80878cad92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mSARSA_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: SARSA_n() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "SARSA_n(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-leanring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlenv",
   "language": "python",
   "name": "rlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
