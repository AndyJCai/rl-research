{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/IPython/core/magics/pylab.py:159: UserWarning: pylab import has clobbered these variables: ['gamma']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  warn(\"pylab import has clobbered these variables: %s\"  % clobbered +\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "sns.set_style(\"darkgrid\")\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windy grid\n",
    "rewardSize = -1\n",
    "gamma = 0.95 # discount rate\n",
    "\n",
    "alpha = 0.85 # learning rate\n",
    "epsilon = 0.9 # e-greedy \n",
    "episodeNum = 100 # number of episodes \n",
    "maxSteps = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_gridworlds\n",
    "from lib.envs.windy_gridworld import WindyGridworldEnv\n",
    "\n",
    "env = WindyGridworldEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to choose the next action \n",
    "@deprecate\n",
    "def choose_action(state, Q): \n",
    "    action=None\n",
    "    if np.random.uniform(0, 1) < epsilon: \n",
    "        action = env.action_space.sample()\n",
    "    else: \n",
    "        action = np.argmax(Q[state, :]) \n",
    "    return action \n",
    "\n",
    "def createEpsilonGreedyPolicy(Q, epsilon, num_actions): \n",
    "    \"\"\" \n",
    "    Creates an epsilon-greedy policy based \n",
    "    on a given Q-function and epsilon. \n",
    "       \n",
    "    Returns a function that takes the state \n",
    "    as an input and returns the probabilities \n",
    "    for each action in the form of a numpy array  \n",
    "    of length of the action space(set of possible actions). \n",
    "    \"\"\"\n",
    "    def policyFunction(state): \n",
    "   \n",
    "        Action_probabilities = np.ones(num_actions, \n",
    "                dtype = float) * epsilon / num_actions \n",
    "                  \n",
    "        best_action = np.argmax(Q[state]) \n",
    "        Action_probabilities[best_action] += (1.0 - epsilon) \n",
    "        return Action_probabilities \n",
    "   \n",
    "    return policyFunction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/montecarlo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Monte_Carlo():\n",
    "    #Initializing the Q-matrix and Returns matrix\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n)) \n",
    "    policy = createEpsilonGreedyPolicy(Q, epsilon, env.action_space.n)\n",
    "    \n",
    "    returns_sum = defaultdict(float)\n",
    "    returns_count = defaultdict(float)\n",
    "    \n",
    "    for i in range(episodeNum):\n",
    "        episode = [] # current episode\n",
    "        state = env.reset()\n",
    "        for t in range(maxSteps):\n",
    "            action_probs = policy(state)\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p = action_probs) \n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "            \n",
    "            sa_in_episode = set([(tuple(x[0]), x[1]) for x in episode])\n",
    "            \n",
    "    for state, action in sa_in_episode:\n",
    "        sa_pair = (state, action)\n",
    "        first_occur_index = next(i for i, x in enumerate(episode) if (x[0] == state and x[1] == action))\n",
    "        G = np.sum([gamma**i * x[2] for i,x in enumerate(episode[first_occur_index:])])\n",
    "        returns_sum[sa_pair] += G\n",
    "        returns_count[sa_pair] += 1.0\n",
    "        \n",
    "        Q[state][action] = returns_sum[sa_pair]/returns_count[sa_pair]\n",
    "                \n",
    "    return Q, policy\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-113b1e63f528>:11: DeprecationWarning: `choose_action` is deprecated!\n",
      "  action1 = choose_action(state1, Q)\n",
      "<ipython-input-13-113b1e63f528>:19: DeprecationWarning: `choose_action` is deprecated!\n",
      "  action2 = choose_action(state2, Q)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-18.63634439, -18.66231957, -17.7394141 , -17.52124722],\n",
       "       [-18.66486933, -17.95019646, -18.34405074, -18.56481229],\n",
       "       [-18.57295776, -18.33697067, -17.76028073, -18.59881721],\n",
       "       [-18.69490531, -18.58561998, -18.33855152, -18.50015535],\n",
       "       [-18.54511456, -17.11211974, -18.53469865, -18.6088891 ],\n",
       "       [-17.66784954, -17.31736788, -18.08484055, -18.54456387],\n",
       "       [-17.64174285, -17.5491478 , -16.65484569, -18.12914468],\n",
       "       [-17.53999577, -14.82364745, -17.75473842, -16.76915042],\n",
       "       [-13.77701632, -15.8570239 , -13.48483142, -17.68957685],\n",
       "       [-17.32282071, -13.79366307, -10.94864536, -16.22185247],\n",
       "       [-18.23736134, -18.52720879, -17.66632354, -17.92425355],\n",
       "       [-18.50386551, -18.36199495, -18.58138025, -18.45141088],\n",
       "       [-18.61884328, -17.74119842, -18.14032902, -18.09073347],\n",
       "       [-18.53101441, -17.86059127, -18.31546796, -18.70789389],\n",
       "       [-17.98603628, -17.09812781, -17.74893768, -18.19343595],\n",
       "       [-16.97639474, -16.71904649, -13.69748998, -17.25606005],\n",
       "       [-14.84313027, -14.08976954,   0.        , -15.45516815],\n",
       "       [-15.19987183, -12.14929231, -16.03106371, -13.72579454],\n",
       "       [-14.24889168, -15.19246965, -15.3934384 , -13.10310276],\n",
       "       [-16.87339373, -13.38093883,  -5.61581105, -16.39091656],\n",
       "       [-17.35684246, -18.23540914, -17.58188108, -16.2826332 ],\n",
       "       [-17.88144876, -18.51171264, -17.83391646, -18.31598386],\n",
       "       [-17.5354412 , -16.78861965, -18.21972095, -17.1673471 ],\n",
       "       [-18.36577674, -18.36097288, -18.45124706, -18.44119465],\n",
       "       [-18.50283622,  -8.10584342, -17.4520964 , -18.47038788],\n",
       "       [-17.44988552,  -1.01493719, -12.70284916, -17.96076517],\n",
       "       [  0.        ,   0.        ,   0.        , -15.62684477],\n",
       "       [-13.30260924, -14.74580189,   0.        ,   0.        ],\n",
       "       [-15.11940429, -14.32421825, -13.83364334,  -3.00997466],\n",
       "       [-12.96073518, -12.99074939,  -5.21767617, -14.11468321],\n",
       "       [-18.44337967, -18.19583435, -16.24265088, -16.86997666],\n",
       "       [-16.7981488 , -10.02301203, -17.30027994, -17.65304527],\n",
       "       [-17.08580095, -13.37241923, -14.9989289 , -17.803754  ],\n",
       "       [-18.56069263, -14.62699374, -18.0751873 , -18.09875131],\n",
       "       [-16.83390749, -13.64998689, -14.82613115, -17.51560828],\n",
       "       [-14.90053915,   0.        ,   0.        ,  -0.85      ],\n",
       "       [  0.        ,  -6.87742745,   0.        ,  -0.85      ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [-16.30148268,  -4.17744807, -11.87015518,  -0.9775    ],\n",
       "       [ -4.797874  ,  -1.78595625,  -1.18271842, -12.66888029],\n",
       "       [-18.21159571, -16.55253287, -16.76581376, -17.22543111],\n",
       "       [-16.82450438, -17.83319936, -16.56048387, -16.31399325],\n",
       "       [-17.30384098, -14.60405753, -16.03645935, -13.73835281],\n",
       "       [-17.46353621, -14.53099027,  -5.70940198, -17.6336178 ],\n",
       "       [-13.56319533, -11.7455527 , -15.69930999, -18.2500999 ],\n",
       "       [ -2.19357906,  -0.9775    ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,  -0.85      ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [ -2.87159042,  -0.9775    ,  -0.85      ,   0.        ],\n",
       "       [-16.92481305, -16.37316585, -16.29840613, -16.52762094],\n",
       "       [-16.46393467, -14.87572954, -15.986466  , -17.19791987],\n",
       "       [-16.91999505, -16.73573758, -15.40239785, -16.67582792],\n",
       "       [-18.4453444 ,  -4.56133672,  -7.08985275, -15.20467556],\n",
       "       [-12.88374288,  -1.09958125,   0.        ,  -9.71089201],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,  -0.85      ],\n",
       "       [  0.        ,  -0.85      ,   0.        ,  -0.85      ],\n",
       "       [-15.96115364, -16.37899385, -16.13745767, -15.71603629],\n",
       "       [-13.64044847, -16.54775811, -16.02430201, -15.61807059],\n",
       "       [-16.7403006 , -14.6560211 , -16.13744283, -15.5390707 ],\n",
       "       [-17.10376655,  -2.64775287,  -9.72932815, -14.72240434],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def SARSA_0():\n",
    "    #Initializing the Q-matrix\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n)) \n",
    "    #Initializing the reward  \n",
    "    reward=0\n",
    "\n",
    "    # Starting the SARSA learning \n",
    "    for episode in range(episodeNum): \n",
    "        t = 0\n",
    "        state1 = env.reset()\n",
    "        action1 = choose_action(state1, Q) \n",
    "\n",
    "\n",
    "        while t < maxSteps: \n",
    "            #Getting the next state \n",
    "            state2, reward, done, info = env.step(action1) \n",
    "\n",
    "            #Choosing the next action \n",
    "            action2 = choose_action(state2, Q) \n",
    "            \n",
    "            #Learning the Q-value \n",
    "            predict = Q[state1, action1] \n",
    "            target = reward + gamma * Q[state2, action2] \n",
    "            Q[state1, action1] = Q[state1, action1] + alpha * (target - predict)\n",
    "\n",
    "            state1 = state2 \n",
    "            action1 = action2 \n",
    "\n",
    "            #Updating the respective vaLues \n",
    "            t += 1\n",
    "            reward += rewardSize\n",
    "\n",
    "            #If at the end of learning process \n",
    "            if done: \n",
    "                break\n",
    "\n",
    "    return Q\n",
    "SARSA_0()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to learn the Q-value \n",
    "def SARSA_update(state, state_n, reward, action, action_n): \n",
    "    predict = Q[state, action] \n",
    "    target = reward + gamma * Q[state2, action2] \n",
    "    \n",
    "    Q[state, action] = Q[state, action] + alpha * (target - predict) \n",
    "    \n",
    "def SARSA_update(state1, state_n, n, action, action_n):\n",
    "    predict = Q[state1, action1]\n",
    "    target = gamma**n * Q[state_n, action_n]\n",
    "    n -= 1\n",
    "    for i in range(n):\n",
    "        state_new, reward, done, info = env.step(action1)\n",
    "        action_new = choose_action(state_new)\n",
    "        action1 = action_new\n",
    "        target += gamma**i * reward\n",
    "        \n",
    "    Q[state, action] = Q[state, action] + alpha * (target - predict) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://miro.medium.com/max/1400/1*-v5wbqLYCvzrOQE2Zmv05g.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SARSA_n(n):\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n)) \n",
    "    #Initializing the reward  \n",
    "    total_reward=0\n",
    "    max_reward = 0\n",
    "\n",
    "    # Starting the SARSA learning \n",
    "    for episode in range(episodeNum): \n",
    "        t = 0\n",
    "        tau = 0\n",
    "        T = sys.maxsize\n",
    "        stored_actions = {}\n",
    "        stored_rewards = {}\n",
    "        stored_states = {}\n",
    "        reward_episode = 0\n",
    "        \n",
    "        stored_states[0] = state = env.reset()\n",
    "        stored_actions[0] = choose_action(state, Q)\n",
    "        \n",
    "        \n",
    "        while (tau < T - 1):\n",
    "            if (t < T):\n",
    "                action_t = choose_action(state=stored_states[t % n], Q=Q)\n",
    "                stored_states[(t+1) % n], stored_rewards[(t+1) % n], done, info = env.step(action_t) \n",
    "                state_t = stored_states[(t+1) % n]\n",
    "                \n",
    "                reward_episode += stored_rewards[(t+1) % n]\n",
    "                total_reward += stored_rewards[(t+1) % n]\n",
    "                \n",
    "                if (done):\n",
    "                    T = t + 1\n",
    "                else:\n",
    "                    stored_actions[(t+1) % n] = choose_action(stored_states[t % n], Q)\n",
    "                \n",
    "                    \n",
    "            tau = t - n + 1\n",
    "            G = 0\n",
    "            if (tau >= 0):\n",
    "                for i in range(tau+1, min(tau+n, T)+1):\n",
    "                    G += gamma ** (i - tau - 1) * stored_rewards[i % n] \n",
    "                if (tau + n < T):\n",
    "                    G += gamma ** n * Q[stored_states[(tau + n) % n]][stored_actions[(tau + n) % n]]\n",
    "                current_Q = Q[stored_states[tau % n]][stored_actions[tau % n]]\n",
    "                Q[stored_states[tau % n]][stored_actions[tau % n]] += alpha * (G - current_Q)\n",
    "                \n",
    "            t += 1\n",
    "                \n",
    "        max_reward = max(reward_episode, max_reward)\n",
    "        \n",
    "    return Q, max_reward, total_reward/episodeNum\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-af4f519ee4ef>:18: DeprecationWarning: `choose_action` is deprecated!\n",
      "  stored_actions[0] = choose_action(state, Q)\n",
      "<ipython-input-15-af4f519ee4ef>:23: DeprecationWarning: `choose_action` is deprecated!\n",
      "  action_t = choose_action(state=stored_states[t % n], Q=Q)\n",
      "<ipython-input-15-af4f519ee4ef>:33: DeprecationWarning: `choose_action` is deprecated!\n",
      "  stored_actions[(t+1) % n] = choose_action(stored_states[t % n], Q)\n"
     ]
    }
   ],
   "source": [
    "Q, max_reward, average_reward = SARSA_n(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -19.99999999 -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-19.99999997 -20.         -20.         -19.99999996]\n",
      " [-19.53165902 -16.22968511 -18.36298961   0.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [ -9.28576003 -19.9997912  -19.97836734 -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-19.99997898 -19.99988849 -19.9994085  -19.99999399]\n",
      " [-17.51563999 -14.27809403 -18.36298961 -17.51563999]\n",
      " [-20.         -19.99999996 -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.          -8.72185267 -18.91138546 -19.79417624]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-19.99523823 -19.97474075 -19.94182362 -19.28923628]\n",
      " [-16.22968511  -6.82147203  -6.82147203 -11.31632003]\n",
      " [ -8.02526122  -8.02526122  -8.02526122  -8.02526122]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [ -7.87026516 -13.94870074 -18.44770204 -17.38835926]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-19.99998615 -19.99999739 -19.99999087 -19.99999991]\n",
      " [-17.51563999 -19.53165902 -16.22968511 -11.31632003]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [ -9.05611159  -3.63078347 -16.61771989 -15.282932  ]\n",
      " [ -2.18227205  -2.01942552 -16.92967366  -2.08416909]\n",
      " [ -2.78298973  -3.85481686  -5.03961291  -3.74749597]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-19.99974318 -19.99686235 -19.97474075 -19.99686235]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [-14.27809403 -18.92133064 -17.51563999 -17.51563999]\n",
      " [ -4.65665271 -19.9872312   -8.6469928  -15.70252656]\n",
      " [ -8.04279322  -5.79802125  -5.39691605  -8.52011712]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [-16.71513696 -19.84308122 -10.39340405 -13.64807129]\n",
      " [ -7.35508842  -5.78179597  -7.17791989 -18.02016218]]\n"
     ]
    }
   ],
   "source": [
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-leanring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://www.cse.unsw.edu.au/~cs9417ml/RL1/images/qalg.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning():\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n)) \n",
    "    policy = createEpsilonGreedyPolicy(Q, epsilon, env.action_space.n)\n",
    "    for episde in range(episodeNum):\n",
    "        state = env.reset()\n",
    "        t = 0\n",
    "        while (t < maxSteps):\n",
    "            action_probs = policy(state)\n",
    "            # choose action according to  \n",
    "            # the probability distribution \n",
    "            action = np.random.choice(np.arange(len(action_probs)), p = action_probs) \n",
    "            \n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            next_best_action = np.argmax(Q[next_state, :])\n",
    "            target = reward + gamma * Q[next_state, next_best_action]\n",
    "            Q[state, action] += alpha * (target - Q[state, action])\n",
    "            \n",
    "            if (done):\n",
    "                break\n",
    "            \n",
    "            state = next_state\n",
    "            t += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-step Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning_n(n: int):\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    policy = createEpsilonGreedyPolicy(Q, epsilon, env.action_space.n)\n",
    "    #Initializing the reward  \n",
    "    total_reward=0\n",
    "    max_reward = 0\n",
    "\n",
    "    # Starting the SARSA learning \n",
    "    for episode in range(episodeNum): \n",
    "        t = 0\n",
    "        tau = 0\n",
    "        T = sys.maxsize\n",
    "        stored_actions = {}\n",
    "        stored_rewards = {}\n",
    "        stored_states = {}\n",
    "        reward_episode = 0\n",
    "        \n",
    "        stored_states[0] = state = env.reset()\n",
    "        stored_actions[0] = choose_action(state, Q)\n",
    "        \n",
    "        \n",
    "        while (tau < T - 1):\n",
    "            if (t < T):\n",
    "                action_probs = policy(stored_states[t % n])\n",
    "                action_t = np.random.choice(np.arange(len(action_probs)), p = action_probs) \n",
    "            \n",
    "                \n",
    "                stored_states[(t+1) % n], stored_rewards[(t+1) % n], done, info = env.step(action_t) \n",
    "                state_t = stored_states[(t+1) % n]\n",
    "                \n",
    "                reward_episode += stored_rewards[(t+1) % n]\n",
    "                total_reward += stored_rewards[(t+1) % n]\n",
    "                \n",
    "                if (done):\n",
    "                    T = t + 1\n",
    "                else:\n",
    "                    stored_actions[(t+1) % n] = np.argmax(Q[state_t, :])\n",
    "                \n",
    "                    \n",
    "            tau = t - n + 1\n",
    "            G = 0\n",
    "            if (tau >= 0):\n",
    "                for i in range(tau+1, min(tau+n, T)+1):\n",
    "                    G += gamma ** (i - tau - 1) * stored_rewards[i % n] \n",
    "\n",
    "                if (tau + n < T):\n",
    "                    G += gamma ** n * Q[stored_states[(tau + n) % n]][stored_actions[(tau + n) % n]]\n",
    "                current_Q = Q[stored_states[tau % n]][stored_actions[tau % n]]\n",
    "                Q[stored_states[tau % n]][stored_actions[tau % n]] += alpha * (G - current_Q)\n",
    "                \n",
    "            t += 1\n",
    "                \n",
    "        max_reward = max(reward_episode, max_reward)\n",
    "        \n",
    "    return Q, max_reward, total_reward/episodeNum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-c60733e64275>:19: DeprecationWarning: `choose_action` is deprecated!\n",
      "  stored_actions[0] = choose_action(state, Q)\n"
     ]
    }
   ],
   "source": [
    "Q, max_reward, average_reward = Q_learning_n(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-19.99999925 -19.99999887 -19.99999887 -19.99999887]\n",
      " [-19.53165902 -19.28923628 -19.28923628 -19.28923628]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [ -6.19547347 -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-19.99961025 -19.99961025 -19.99961025 -19.9994085 ]\n",
      " [-18.92133064 -18.92133064 -18.36298961 -18.36298961]\n",
      " [-19.99999968 -19.99999968 -19.99999968 -19.99999968]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [ -7.50343786 -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-19.86600985 -19.86600985 -19.86600985 -19.86600985]\n",
      " [-16.22968511 -14.27809403 -14.27809403 -14.27809403]\n",
      " [ -8.02526122  -8.0246518   -8.0251698   -8.0252475 ]\n",
      " [-19.99951911 -20.         -20.         -20.        ]\n",
      " [ -4.1802522  -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-19.99961025 -19.9994085  -19.9994085  -19.9994085 ]\n",
      " [-18.92133064 -18.92133064 -18.92133064 -18.36298961]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [-18.53838636 -18.36298961 -17.93413671 -18.36298961]\n",
      " [-19.83257025 -19.83239816 -19.81880293  -3.70507193]\n",
      " [-13.00348953  -3.93934005  -4.55195551 -18.06213654]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-19.99686235 -19.99686235 -19.99523823 -19.99523823]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [-16.22968511 -16.22968511 -15.23792888 -14.27809403]\n",
      " [-19.98335601 -19.98335601 -19.98242805 -19.97864478]\n",
      " [-19.9019272  -19.95942499 -17.15013131 -19.92631031]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -19.99999999 -19.99999999 -19.99999999]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [ -9.8330285  -19.28923628 -19.28140296 -19.13877802]\n",
      " [-19.97063796 -13.65904385 -13.9748031  -14.27051256]]\n"
     ]
    }
   ],
   "source": [
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlenv",
   "language": "python",
   "name": "rlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
