{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "sns.set_style(\"darkgrid\")\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windy grid\n",
    "rewardSize = -1\n",
    "gamma = 0.95 # discount rate\n",
    "\n",
    "alpha = 0.85 # learning rate\n",
    "epsilon = 0.9 # e-greedy \n",
    "episodeNum = 100 # number of episodes \n",
    "maxSteps = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_gridworlds\n",
    "from lib.envs.windy_gridworld import WindyGridworldEnv\n",
    "\n",
    "env = WindyGridworldEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to choose the next action \n",
    "def choose_action(state, Q): \n",
    "    action=None\n",
    "    if np.random.uniform(0, 1) < epsilon: \n",
    "        action = env.action_space.sample()\n",
    "    else: \n",
    "        action = np.argmax(Q[state, :]) \n",
    "    return action \n",
    "\n",
    "def createEpsilonGreedyPolicy(Q, epsilon, num_actions): \n",
    "    \"\"\" \n",
    "    Creates an epsilon-greedy policy based \n",
    "    on a given Q-function and epsilon. \n",
    "       \n",
    "    Returns a function that takes the state \n",
    "    as an input and returns the probabilities \n",
    "    for each action in the form of a numpy array  \n",
    "    of length of the action space(set of possible actions). \n",
    "    \"\"\"\n",
    "    def policyFunction(state): \n",
    "   \n",
    "        Action_probabilities = np.ones(num_actions, \n",
    "                dtype = float) * epsilon / num_actions \n",
    "                  \n",
    "        best_action = np.argmax(Q[state]) \n",
    "        Action_probabilities[best_action] += (1.0 - epsilon) \n",
    "        return Action_probabilities \n",
    "   \n",
    "    return policyFunction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/montecarlo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Monte_Carlo():\n",
    "    #Initializing the Q-matrix and Returns matrix\n",
    "    policy = createEpsilonGreedyPolicy(Q, epsilon, env.action_space.n)\n",
    "    \n",
    "    returns_sum = defaultdict(float)\n",
    "    returns_count = defaultdict(float)\n",
    "    \n",
    "    for i in range(episodeNum):\n",
    "        episode = [] # current episode\n",
    "        state = env.reset()\n",
    "        for t in range(maxSteps):\n",
    "            action_probs = policy(state)\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p = action_probs) \n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "            \n",
    "            sa_in_episode = set([(tuple(x[0]), x[1]) for x in episode])\n",
    "            \n",
    "    for state, action in sa_in_episode:\n",
    "        sa_pair = (state, action)\n",
    "        first_occur_index = next(i for i, x in enumerate(episode) if (x[0] == state and x[1] == action))\n",
    "        G = np.sum([])\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-18.33574913, -17.42376719, -17.56174589, -17.63852137],\n",
       "       [-18.24509149, -18.77059763, -15.19689868, -18.15352496],\n",
       "       [-18.70443   , -18.58418828, -18.72741887, -18.46015189],\n",
       "       [-18.90201731, -18.98671894, -18.91727076, -18.54381478],\n",
       "       [-18.89179623, -18.99896306, -18.83359513, -18.91551547],\n",
       "       [-18.72341235, -18.92186913, -18.95285582, -18.98546935],\n",
       "       [-17.50273806, -18.99231009, -18.69686597, -18.99133498],\n",
       "       [-18.38582386, -14.2696141 , -18.53097014, -18.77284933],\n",
       "       [-17.23789216, -11.69580801, -17.10007082, -17.73836444],\n",
       "       [-16.51125096, -16.24819263, -10.68012384, -10.07629561],\n",
       "       [-18.45955432, -18.15493956, -16.2899608 , -18.15996949],\n",
       "       [-18.21915125, -18.4583135 , -17.78521901, -18.19330664],\n",
       "       [-18.58296151, -18.16145592, -18.36392481, -17.13538854],\n",
       "       [-18.78187765, -18.48353909, -18.54863664, -18.43628347],\n",
       "       [-18.5069888 , -18.55071849, -16.15224826, -18.92744865],\n",
       "       [-18.47427044, -18.32772669, -17.81096417, -18.6146029 ],\n",
       "       [  0.        , -11.91964644, -15.54707613, -12.52880191],\n",
       "       [ -8.89938855, -12.88636988, -17.01096862, -15.16568585],\n",
       "       [-16.6810068 , -14.81486316, -15.49843723, -17.35146444],\n",
       "       [-17.29111641, -15.1795788 ,  -5.52223694, -16.67619564],\n",
       "       [-18.57922676, -16.00569219, -16.66537341, -17.10740634],\n",
       "       [-17.98820966, -15.71580003, -15.03108522, -15.28376737],\n",
       "       [-17.46779799, -18.32499449, -16.48355103, -15.68560837],\n",
       "       [-18.76254007, -18.3865219 , -17.96578336, -18.32648975],\n",
       "       [-18.82723482, -16.88706866, -16.87132182, -17.9426655 ],\n",
       "       [-17.8925885 ,  -0.996625  , -13.60661507,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [ -7.13664618, -13.77292953,  -0.85      , -16.05296092],\n",
       "       [-16.21936709,  -9.9639023 ,  -3.05146345, -10.69379329],\n",
       "       [-12.00364854,  -6.6176617 ,  -3.61010641, -14.06809128],\n",
       "       [-15.79249481, -16.03679309, -16.75787394, -16.63817416],\n",
       "       [-17.56481414, -17.06800397, -15.89770011, -17.78240297],\n",
       "       [-17.78135648, -18.41778422, -16.3407749 , -16.71039888],\n",
       "       [-18.77809456, -18.27965109, -17.08204904, -18.02592607],\n",
       "       [-17.89222032,  -3.95844752, -15.54506064, -16.76974149],\n",
       "       [-13.24857101,   0.        ,   0.        , -10.20323733],\n",
       "       [  0.        ,   0.        ,   0.        , -15.3234234 ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [-10.18241726,  -7.97893613,  -1.8889125 ,  -0.99949375],\n",
       "       [ -5.11447507,  -6.07740332,  -2.60387965,  -2.54989865],\n",
       "       [-15.8250697 , -14.92564311, -13.88488442, -15.6372202 ],\n",
       "       [-17.26899991, -17.07737915, -16.17032475, -14.77783476],\n",
       "       [-17.87431155, -12.81514995, -14.36720564, -16.86167538],\n",
       "       [-18.67180031, -15.56610865, -17.96582461, -17.3113031 ],\n",
       "       [-14.83051185,  -1.13333687, -14.98660985, -14.26953222],\n",
       "       [  0.        , -11.218909  ,  -1.536375  ,  -3.55021812],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [ -1.96836811, -11.13451092,  -2.53817792,  -0.85      ],\n",
       "       [ -2.95045049,  -3.46762603,  -1.663875  ,  -1.04297243],\n",
       "       [-16.52512537, -15.3966021 , -15.64162096, -15.808259  ],\n",
       "       [-15.85899706, -12.82883547, -10.54753971, -15.8752851 ],\n",
       "       [-17.1043901 , -18.64035875, -14.59960029, -12.45045948],\n",
       "       [-17.28265849,  -4.01210045, -14.74719824, -16.3426494 ],\n",
       "       [-16.83731446,  -1.06448578,  -3.01400761, -17.91951497],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [ -3.70921299,  -0.85      ,  -0.85      ,   0.        ],\n",
       "       [-16.38637266, -13.94523206, -15.3960732 , -13.14727917],\n",
       "       [-15.20541523, -14.74958507, -14.69143229, -15.11744939],\n",
       "       [-10.23369424, -15.64688796, -15.37962019, -11.39494325],\n",
       "       [-14.2327967 , -13.43620314,  -9.78177441, -15.73223957],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [ -0.85      ,   0.        ,  -0.85      ,   0.        ]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def SARSA_0():\n",
    "    #Initializing the Q-matrix\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n)) \n",
    "    #Initializing the reward  \n",
    "    reward=0\n",
    "\n",
    "    # Starting the SARSA learning \n",
    "    for episode in range(episodeNum): \n",
    "        t = 0\n",
    "        state1 = env.reset()\n",
    "        action1 = choose_action(state1, Q) \n",
    "\n",
    "\n",
    "        while t < maxSteps: \n",
    "            #Getting the next state \n",
    "            state2, reward, done, info = env.step(action1) \n",
    "\n",
    "            #Choosing the next action \n",
    "            action2 = choose_action(state2, Q) \n",
    "            \n",
    "            #Learning the Q-value \n",
    "            predict = Q[state1, action1] \n",
    "            target = reward + gamma * Q[state2, action2] \n",
    "            Q[state1, action1] = Q[state1, action1] + alpha * (target - predict)\n",
    "\n",
    "            state1 = state2 \n",
    "            action1 = action2 \n",
    "\n",
    "            #Updating the respective vaLues \n",
    "            t += 1\n",
    "            reward += rewardSize\n",
    "\n",
    "            #If at the end of learning process \n",
    "            if done: \n",
    "                break\n",
    "\n",
    "    return Q\n",
    "SARSA_0()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to learn the Q-value \n",
    "def SARSA_update(state, state_n, reward, action, action_n): \n",
    "    predict = Q[state, action] \n",
    "    target = reward + gamma * Q[state2, action2] \n",
    "    \n",
    "    Q[state, action] = Q[state, action] + alpha * (target - predict) \n",
    "    \n",
    "def SARSA_update(state1, state_n, n, action, action_n):\n",
    "    predict = Q[state1, action1]\n",
    "    target = gamma**n * Q[state_n, action_n]\n",
    "    n -= 1\n",
    "    for i in range(n):\n",
    "        state_new, reward, done, info = env.step(action1)\n",
    "        action_new = choose_action(state_new)\n",
    "        action1 = action_new\n",
    "        target += gamma**i * reward\n",
    "        \n",
    "    Q[state, action] = Q[state, action] + alpha * (target - predict) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://miro.medium.com/max/1400/1*-v5wbqLYCvzrOQE2Zmv05g.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SARSA_n(n):\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n)) \n",
    "    #Initializing the reward  \n",
    "    total_reward=0\n",
    "    max_reward = 0\n",
    "\n",
    "    # Starting the SARSA learning \n",
    "    for episode in range(episodeNum): \n",
    "        t = 0\n",
    "        tau = 0\n",
    "        T = sys.maxsize\n",
    "        stored_actions = {}\n",
    "        stored_rewards = {}\n",
    "        stored_states = {}\n",
    "        reward_episode = 0\n",
    "        \n",
    "        stored_states[0] = state = env.reset()\n",
    "        stored_actions[0] = choose_action(state, Q)\n",
    "        \n",
    "        \n",
    "        while (tau < T - 1):\n",
    "            if (t < T):\n",
    "                action_t = choose_action(state=stored_states[t % n], Q=Q)\n",
    "                stored_states[(t+1) % n], stored_rewards[(t+1) % n], done, info = env.step(action_t) \n",
    "                state_t = stored_states[(t+1) % n]\n",
    "                \n",
    "                reward_episode += stored_rewards[(t+1) % n]\n",
    "                total_reward += stored_rewards[(t+1) % n]\n",
    "                \n",
    "                if (done):\n",
    "                    T = t + 1\n",
    "                else:\n",
    "                    stored_actions[(t+1) % n] = choose_action(stored_states[t % n], Q)\n",
    "                \n",
    "                    \n",
    "            tau = t - n + 1\n",
    "            G = 0\n",
    "            if (tau >= 0):\n",
    "                for i in range(tau+1, min(tau+n, T)+1):\n",
    "                    G += gamma ** (i - tau - 1) * stored_rewards[i % n] \n",
    "                if (tau + n < T):\n",
    "                    G += gamma ** n * Q[stored_states[(tau + n) % n]][stored_actions[(tau + n) % n]]\n",
    "                current_Q = Q[stored_states[tau % n]][stored_actions[tau % n]]\n",
    "                Q[stored_states[tau % n]][stored_actions[tau % n]] += alpha * (G - current_Q)\n",
    "                \n",
    "            t += 1\n",
    "                \n",
    "        max_reward = max(reward_episode, max_reward)\n",
    "        \n",
    "    return Q, max_reward, total_reward/episodeNum\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-19.99999643 -19.9999986  -19.99999661 -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-18.92133064 -19.69139777 -19.53165902 -18.92133064]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-19.99548465 -20.         -20.         -19.80117864]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-19.99983078 -19.99999399 -19.99863768 -19.99999828]\n",
      " [-18.92133064 -18.36298961 -11.31632003 -18.36298961]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-19.99999998 -19.53252089 -16.60355271 -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-19.98903284 -19.96166605 -19.86600985 -19.96166605]\n",
      " [-17.51563999  -6.82147203 -11.31632003 -14.27809403]\n",
      " [ -8.02526122  -8.02526122  -8.02526122  -8.02526122]\n",
      " [-20.         -19.57810011 -20.         -17.619403  ]\n",
      " [-19.99968306 -19.93859503 -18.27366755 -11.73954205]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-19.99999951 -19.99999968 -19.99999087 -19.99999887]\n",
      " [-18.36298961 -17.51563999 -19.53165902 -18.92133064]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [ -2.14426601  -8.98568466  -3.08528113  -3.54450661]\n",
      " [ -2.68706004  -9.45270784  -1.38003051  -3.37955887]\n",
      " [-11.17058379  -3.08726177  -3.77442178 -10.31611744]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-19.99910233 -19.91171035 -19.99983078 -19.9994085 ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [-18.36298961  -6.82147203 -12.195877   -17.51563999]\n",
      " [-13.14961607 -19.9923595  -15.39087797  -9.86610412]\n",
      " [ -8.54964937 -19.99670328  -4.03820911  -7.87407054]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [-20.         -20.         -20.         -20.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [ -3.79258741 -18.64594271  -5.98236629 -17.5976907 ]\n",
      " [-14.28074162 -19.27919963  -7.06069823  -3.35658757]]\n",
      "0\n",
      "-9513.09\n"
     ]
    }
   ],
   "source": [
    "Q, max_reward, average_reward = SARSA_n(10)\n",
    "print(Q)\n",
    "print(max_reward)\n",
    "print(average_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-leanring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://www.cse.unsw.edu.au/~cs9417ml/RL1/images/qalg.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning():\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n)) \n",
    "    policy = createEpsilonGreedyPolicy(Q, epsilon, env.action_space.n)\n",
    "    for episde in range(episodeNum):\n",
    "        state = env.reset()\n",
    "        t = 0\n",
    "        while (t < maxSteps):\n",
    "            action_probs = policy(state)\n",
    "            # choose action according to  \n",
    "            # the probability distribution \n",
    "            action = np.random.choice(np.arange(len(action_probs)), p = action_probs) \n",
    "            \n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            next_best_action = np.argmax(Q[next_state, :])\n",
    "            target = reward + gamma * Q[next_state, next_best_action]\n",
    "            Q[state, action] += alpha * (target - Q[state, action])\n",
    "            \n",
    "            if (done):\n",
    "                break\n",
    "            \n",
    "            state = next_state\n",
    "            t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlenv",
   "language": "python",
   "name": "rlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
